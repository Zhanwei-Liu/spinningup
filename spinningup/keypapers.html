

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>深度强化学习的核心论文 &mdash; Spinning Up  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/openai_icon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/modify.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="练习" href="exercises.html" />
    <link rel="prev" title="深度强化学习研究者的资料" href="spinningup.html" /> 
 <script type="text/javascript">
 
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?7e494634f392b55baa85cfd2b508ae23";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();

 
 </script> 


  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/spinning-up-logo2.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">用户文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user/introduction.html">项目介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/algorithms.html">核心算法及其实现</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/running.html">运行试验</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/saving_and_loading.html">试验输出</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/plotting.html">绘制结果</a></li>
</ul>
<p class="caption"><span class="caption-text">强化学习介绍</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="rl_intro.html">第一部分：强化学习中的核心概念</a></li>
<li class="toctree-l1"><a class="reference internal" href="rl_intro2.html">第二部分：强化学习算法概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="rl_intro3.html">第三部分：策略优化介绍</a></li>
</ul>
<p class="caption"><span class="caption-text">资源</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="spinningup.html">深度强化学习研究者的资料</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">深度强化学习的核心论文</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#section-2">1. 免模型强化学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#a-q-learning">a. 深度 Q-learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#b">b. 策略梯度</a></li>
<li class="toctree-l3"><a class="reference internal" href="#c">c. 确定策略梯度</a></li>
<li class="toctree-l3"><a class="reference internal" href="#d">d. 分布式强化学习</a></li>
<li class="toctree-l3"><a class="reference internal" href="#e">e. 动作依赖性策略梯度</a></li>
<li class="toctree-l3"><a class="reference internal" href="#f-path-consistency-learning">f. 路径一致性学习(Path-Consistency Learning)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#g-other-directions-for-combining-policy-learning-and-q-learning">g. Other Directions for Combining Policy-Learning and Q-Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#h-evolutionary">h. 进化(Evolutionary)算法</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#exploration">2. 探索(Exploration)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#a-intrinsic-motivation">a. 内在激励(Intrinsic Motivation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#b-1">b. 非监督强化学习</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#section-3">3. 迁移和多任务强化学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hierarchy">4. 层次(Hierarchy)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#memory">5. 记忆(Memory)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#section-4">6. 有模型强化学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#a">a. 模型可被学习</a></li>
<li class="toctree-l3"><a class="reference internal" href="#b-2">b. 模型已知</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#meta-rl">7. 元学习(Meta-RL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#scaling-rl">8. Scaling RL</a></li>
<li class="toctree-l2"><a class="reference internal" href="#section-5">9. 现实世界的强化学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="#section-6">10. 安全</a></li>
<li class="toctree-l2"><a class="reference internal" href="#section-7">11. 模仿学习和逆强化学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="#section-8">12. 可复现、分析和评价</a></li>
<li class="toctree-l2"><a class="reference internal" href="#section-9">13. 额外奖励：强化学习理论的经典论文</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="exercises.html">练习</a></li>
<li class="toctree-l1"><a class="reference internal" href="bench.html">Spinning Up 算法实现的基准</a></li>
</ul>
<p class="caption"><span class="caption-text">算法文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/vpg.html">Vanilla Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/trpo.html">Trust Region Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/ppo.html">Proximal Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/ddpg.html">Deep Deterministic Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/td3.html">Twin Delayed DDPG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/sac.html">Soft Actor-Critic</a></li>
</ul>
<p class="caption"><span class="caption-text">工具文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../utils/logger.html">日志打印</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/plotter.html">绘图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/mpi.html">MPI 工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/run_utils.html">运行工具</a></li>
</ul>
<p class="caption"><span class="caption-text">其他</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../etc/acknowledgements.html">致谢</a></li>
<li class="toctree-l1"><a class="reference internal" href="../etc/author.html">作者</a></li>
<li class="toctree-l1"><a class="reference internal" href="../etc/translator.html">关于译者</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Spinning Up</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>深度强化学习的核心论文</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/Zhanwei-Liu/spinningup/blob/master/docs/spinningup/keypapers.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="section-1">
<h1><a class="toc-backref" href="#toc-entry-1">深度强化学习的核心论文</a><a class="headerlink" href="#section-1" title="Permalink to this headline">¶</a></h1>
<p>What follows is a list of papers in deep RL that are worth reading. This is <em>far</em> from comprehensive, but should provide a useful starting point for someone looking to do research in the field.</p>
<div class="contents topic" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#section-1" id="toc-entry-1">深度强化学习的核心论文</a><ul>
<li><a class="reference internal" href="#section-2" id="toc-entry-2">1. 免模型强化学习</a></li>
<li><a class="reference internal" href="#exploration" id="toc-entry-3">2. 探索(Exploration)</a></li>
<li><a class="reference internal" href="#section-3" id="toc-entry-4">3. 迁移和多任务强化学习</a></li>
<li><a class="reference internal" href="#hierarchy" id="toc-entry-5">4. 层次(Hierarchy)</a></li>
<li><a class="reference internal" href="#memory" id="toc-entry-6">5. 记忆(Memory)</a></li>
<li><a class="reference internal" href="#section-4" id="toc-entry-7">6. 有模型强化学习</a></li>
<li><a class="reference internal" href="#meta-rl" id="toc-entry-8">7. 元学习(Meta-RL)</a></li>
<li><a class="reference internal" href="#scaling-rl" id="toc-entry-9">8. Scaling RL</a></li>
<li><a class="reference internal" href="#section-5" id="toc-entry-10">9. 现实世界的强化学习</a></li>
<li><a class="reference internal" href="#section-6" id="toc-entry-11">10. 安全</a></li>
<li><a class="reference internal" href="#section-7" id="toc-entry-12">11. 模仿学习和逆强化学习</a></li>
<li><a class="reference internal" href="#section-8" id="toc-entry-13">12. 可复现、分析和评价</a></li>
<li><a class="reference internal" href="#section-9" id="toc-entry-14">13. 额外奖励：强化学习理论的经典论文</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="section-2">
<h2><a class="toc-backref" href="#toc-entry-2">1. 免模型强化学习</a><a class="headerlink" href="#section-2" title="Permalink to this headline">¶</a></h2>
<div class="section" id="a-q-learning">
<h3>a. 深度 Q-learning<a class="headerlink" href="#a-q-learning" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a>, Mnih et al, 2013. <strong>Algorithm: DQN.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><a class="reference external" href="https://arxiv.org/abs/1507.06527">Deep Recurrent Q-Learning for Partially Observable MDPs</a>, Hausknecht and Stone, 2015. <strong>Algorithm: Deep Recurrent Q-Learning.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td><a class="reference external" href="https://arxiv.org/abs/1511.06581">Dueling Network Architectures for Deep Reinforcement Learning</a>, Wang et al, 2015. <strong>Algorithm: Dueling DQN.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td><a class="reference external" href="https://arxiv.org/abs/1509.06461">Deep Reinforcement Learning with Double Q-learning</a>, Hasselt et al 2015. <strong>Algorithm: Double DQN.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[5]</td><td><a class="reference external" href="https://arxiv.org/abs/1511.05952">Prioritized Experience Replay</a>, Schaul et al, 2015. <strong>Algorithm: Prioritized Experience Replay (PER).</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[6]</td><td><a class="reference external" href="https://arxiv.org/abs/1710.02298">Rainbow: Combining Improvements in Deep Reinforcement Learning</a>, Hessel et al, 2017. <strong>Algorithm: Rainbow DQN.</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="b">
<h3>b. 策略梯度<a class="headerlink" href="#b" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="footnote-7" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[7]</td><td><a class="reference external" href="https://arxiv.org/abs/1602.01783">Asynchronous Methods for Deep Reinforcement Learning</a>, Mnih et al, 2016. <strong>Algorithm: A3C.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-8" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[8]</td><td><a class="reference external" href="https://arxiv.org/abs/1502.05477">Trust Region Policy Optimization</a>, Schulman et al, 2015. <strong>Algorithm: TRPO.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-9" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[9]</td><td><a class="reference external" href="https://arxiv.org/abs/1506.02438">High-Dimensional Continuous Control Using Generalized Advantage Estimation</a>, Schulman et al, 2015. <strong>Algorithm: GAE.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-10" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[10]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization Algorithms</a>, Schulman et al, 2017. <strong>Algorithm: PPO-Clip, PPO-Penalty.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-11" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[11]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.02286">Emergence of Locomotion Behaviours in Rich Environments</a>, Heess et al, 2017. <strong>Algorithm: PPO-Penalty.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-12" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[12]</td><td><a class="reference external" href="https://arxiv.org/abs/1708.05144">Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation</a>, Wu et al, 2017. <strong>Algorithm: ACKTR.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-13" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[13]</td><td><a class="reference external" href="https://arxiv.org/abs/1611.01224">Sample Efficient Actor-Critic with Experience Replay</a>, Wang et al, 2016. <strong>Algorithm: ACER.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-14" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[14]</td><td><a class="reference external" href="https://arxiv.org/abs/1801.01290">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a>, Haarnoja et al, 2018. <strong>Algorithm: SAC.</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="c">
<h3>c. 确定策略梯度<a class="headerlink" href="#c" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="footnote-15" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[15]</td><td><a class="reference external" href="http://proceedings.mlr.press/v32/silver14.pdf">Deterministic Policy Gradient Algorithms</a>, Silver et al, 2014. <strong>Algorithm: DPG.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-16" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[16]</td><td><a class="reference external" href="https://arxiv.org/abs/1509.02971">Continuous Control With Deep Reinforcement Learning</a>, Lillicrap et al, 2015. <strong>Algorithm: DDPG.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-17" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[17]</td><td><a class="reference external" href="https://arxiv.org/abs/1802.09477">Addressing Function Approximation Error in Actor-Critic Methods</a>, Fujimoto et al, 2018. <strong>Algorithm: TD3.</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="d">
<h3>d. 分布式强化学习<a class="headerlink" href="#d" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="footnote-18" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[18]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.06887">A Distributional Perspective on Reinforcement Learning</a>, Bellemare et al, 2017. <strong>Algorithm: C51.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-19" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[19]</td><td><a class="reference external" href="https://arxiv.org/abs/1710.10044">Distributional Reinforcement Learning with Quantile Regression</a>, Dabney et al, 2017. <strong>Algorithm: QR-DQN.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-20" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[20]</td><td><a class="reference external" href="https://arxiv.org/abs/1806.06923">Implicit Quantile Networks for Distributional Reinforcement Learning</a>, Dabney et al, 2018. <strong>Algorithm: IQN.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-21" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[21]</td><td><a class="reference external" href="https://openreview.net/forum?id=ByG_3s09KX">Dopamine: A Research Framework for Deep Reinforcement Learning</a>, Anonymous, 2018. <strong>Contribution:</strong> Introduces Dopamine, a code repository containing implementations of DQN, C51, IQN, and Rainbow. <a class="reference external" href="https://github.com/google/dopamine">Code link.</a></td></tr>
</tbody>
</table>
</div>
<div class="section" id="e">
<h3>e. 动作依赖性策略梯度<a class="headerlink" href="#e" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="footnote-22" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[22]</td><td><a class="reference external" href="https://arxiv.org/abs/1611.02247">Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic</a>, Gu et al, 2016. <strong>Algorithm: Q-Prop.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-23" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[23]</td><td><a class="reference external" href="https://arxiv.org/abs/1710.11198">Action-depedent Control Variates for Policy Optimization via Stein’s Identity</a>, Liu et al, 2017. <strong>Algorithm: Stein Control Variates.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-24" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[24]</td><td><a class="reference external" href="https://arxiv.org/abs/1802.10031">The Mirage of Action-Dependent Baselines in Reinforcement Learning</a>, Tucker et al, 2018. <strong>Contribution:</strong> interestingly, critiques and reevaluates claims from earlier papers (including Q-Prop and stein control variates) and finds important methodological errors in them.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="f-path-consistency-learning">
<h3>f. 路径一致性学习(Path-Consistency Learning)<a class="headerlink" href="#f-path-consistency-learning" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="footnote-25" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[25]</td><td><a class="reference external" href="https://arxiv.org/abs/1702.08892">Bridging the Gap Between Value and Policy Based Reinforcement Learning</a>, Nachum et al, 2017. <strong>Algorithm: PCL.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-26" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[26]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.01891">Trust-PCL: An Off-Policy Trust Region Method for Continuous Control</a>, Nachum et al, 2017. <strong>Algorithm: Trust-PCL.</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="g-other-directions-for-combining-policy-learning-and-q-learning">
<h3>g. Other Directions for Combining Policy-Learning and Q-Learning<a class="headerlink" href="#g-other-directions-for-combining-policy-learning-and-q-learning" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="footnote-27" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[27]</td><td><a class="reference external" href="https://arxiv.org/abs/1611.01626">Combining Policy Gradient and Q-learning</a>, O’Donoghue et al, 2016. <strong>Algorithm: PGQL.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-28" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[28]</td><td><a class="reference external" href="https://arxiv.org/abs/1704.04651">The Reactor: A Fast and Sample-Efficient Actor-Critic Agent for Reinforcement Learning</a>, Gruslys et al, 2017. <strong>Algorithm: Reactor.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-29" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[29]</td><td><a class="reference external" href="http://papers.nips.cc/paper/6974-interpolated-policy-gradient-merging-on-policy-and-off-policy-gradient-estimation-for-deep-reinforcement-learning">Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning</a>, Gu et al, 2017. <strong>Algorithm: IPG.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-30" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[30]</td><td><a class="reference external" href="https://arxiv.org/abs/1704.06440">Equivalence Between Policy Gradients and Soft Q-Learning</a>, Schulman et al, 2017. <strong>Contribution:</strong> Reveals a theoretical link between these two families of RL algorithms.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="h-evolutionary">
<h3>h. 进化(Evolutionary)算法<a class="headerlink" href="#h-evolutionary" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="footnote-31" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[31]</td><td><a class="reference external" href="https://arxiv.org/abs/1703.03864">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a>, Salimans et al, 2017. <strong>Algorithm: ES.</strong></td></tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="exploration">
<h2><a class="toc-backref" href="#toc-entry-3">2. 探索(Exploration)</a><a class="headerlink" href="#exploration" title="Permalink to this headline">¶</a></h2>
<div class="section" id="a-intrinsic-motivation">
<h3>a. 内在激励(Intrinsic Motivation)<a class="headerlink" href="#a-intrinsic-motivation" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="footnote-32" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[32]</td><td><a class="reference external" href="https://arxiv.org/abs/1605.09674">VIME: Variational Information Maximizing Exploration</a>, Houthooft et al, 2016. <strong>Algorithm: VIME.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-33" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[33]</td><td><a class="reference external" href="https://arxiv.org/abs/1606.01868">Unifying Count-Based Exploration and Intrinsic Motivation</a>, Bellemare et al, 2016. <strong>Algorithm: CTS-based Pseudocounts.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-34" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[34]</td><td><a class="reference external" href="https://arxiv.org/abs/1703.01310">Count-Based Exploration with Neural Density Models</a>, Ostrovski et al, 2017. <strong>Algorithm: PixelCNN-based Pseudocounts.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-35" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[35]</td><td><a class="reference external" href="https://arxiv.org/abs/1611.04717">#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning</a>, Tang et al, 2016. <strong>Algorithm: Hash-based Counts.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-36" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[36]</td><td><a class="reference external" href="https://arxiv.org/abs/1703.01260">EX2: Exploration with Exemplar Models for Deep Reinforcement Learning</a>, Fu et al, 2017. <strong>Algorithm: EX2.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-37" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[37]</td><td><a class="reference external" href="https://arxiv.org/abs/1705.05363">Curiosity-driven Exploration by Self-supervised Prediction</a>, Pathak et al, 2017. <strong>Algorithm: Intrinsic Curiosity Module (ICM).</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-38" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[38]</td><td><a class="reference external" href="https://arxiv.org/abs/1808.04355">Large-Scale Study of Curiosity-Driven Learning</a>, Burda et al, 2018. <strong>Contribution:</strong> Systematic analysis of how surprisal-based intrinsic motivation performs in a wide variety of environments.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-39" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[39]</td><td><a class="reference external" href="https://arxiv.org/abs/1810.12894">Exploration by Random Network Distillation</a>, Burda et al, 2018. <strong>Algorithm: RND.</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="b-1">
<h3>b. 非监督强化学习<a class="headerlink" href="#b-1" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="footnote-40" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[40]</td><td><a class="reference external" href="https://arxiv.org/abs/1611.07507">Variational Intrinsic Control</a>, Gregor et al, 2016. <strong>Algorithm: VIC.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-41" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[41]</td><td><a class="reference external" href="https://arxiv.org/abs/1802.06070">Diversity is All You Need: Learning Skills without a Reward Function</a>, Eysenbach et al, 2018. <strong>Algorithm: DIAYN.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-42" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[42]</td><td><a class="reference external" href="https://arxiv.org/abs/1807.10299">Variational Option Discovery Algorithms</a>, Achiam et al, 2018. <strong>Algorithm: VALOR.</strong></td></tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="section-3">
<h2><a class="toc-backref" href="#toc-entry-4">3. 迁移和多任务强化学习</a><a class="headerlink" href="#section-3" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="footnote-43" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[43]</td><td><a class="reference external" href="https://arxiv.org/abs/1606.04671">Progressive Neural Networks</a>, Rusu et al, 2016. <strong>Algorithm: Progressive Networks.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-44" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[44]</td><td><a class="reference external" href="http://proceedings.mlr.press/v37/schaul15.pdf">Universal Value Function Approximators</a>, Schaul et al, 2015. <strong>Algorithm: UVFA.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-45" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[45]</td><td><a class="reference external" href="https://arxiv.org/abs/1611.05397">Reinforcement Learning with Unsupervised Auxiliary Tasks</a>, Jaderberg et al, 2016. <strong>Algorithm: UNREAL.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-46" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[46]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.03300">The Intentional Unintentional Agent: Learning to Solve Many Continuous Control Tasks Simultaneously</a>, Cabi et al, 2017. <strong>Algorithm: IU Agent.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-47" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[47]</td><td><a class="reference external" href="https://arxiv.org/abs/1701.08734">PathNet: Evolution Channels Gradient Descent in Super Neural Networks</a>, Fernando et al, 2017. <strong>Algorithm: PathNet.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-48" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[48]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.07907">Mutual Alignment Transfer Learning</a>, Wulfmeier et al, 2017. <strong>Algorithm: MATL.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-49" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[49]</td><td><a class="reference external" href="https://openreview.net/forum?id=rk07ZXZRb&amp;noteId=rk07ZXZRb">Learning an Embedding Space for Transferable Robot Skills</a>, Hausman et al, 2018.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-50" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[50]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.01495">Hindsight Experience Replay</a>, Andrychowicz et al, 2017. <strong>Algorithm: Hindsight Experience Replay (HER).</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="hierarchy">
<h2><a class="toc-backref" href="#toc-entry-5">4. 层次(Hierarchy)</a><a class="headerlink" href="#hierarchy" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="footnote-51" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[51]</td><td><a class="reference external" href="https://arxiv.org/abs/1606.04695">Strategic Attentive Writer for Learning Macro-Actions</a>, Vezhnevets et al, 2016. <strong>Algorithm: STRAW.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-52" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[52]</td><td><a class="reference external" href="https://arxiv.org/abs/1703.01161">FeUdal Networks for Hierarchical Reinforcement Learning</a>, Vezhnevets et al, 2017. <strong>Algorithm: Feudal Networks</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-53" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[53]</td><td><a class="reference external" href="https://arxiv.org/abs/1805.08296">Data-Efficient Hierarchical Reinforcement Learning</a>, Nachum et al, 2018. <strong>Algorithm: HIRO.</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="memory">
<h2><a class="toc-backref" href="#toc-entry-6">5. 记忆(Memory)</a><a class="headerlink" href="#memory" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="footnote-54" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[54]</td><td><a class="reference external" href="https://arxiv.org/abs/1606.04460">Model-Free Episodic Control</a>, Blundell et al, 2016. <strong>Algorithm: MFEC.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-55" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[55]</td><td><a class="reference external" href="https://arxiv.org/abs/1703.01988">Neural Episodic Control</a>, Pritzel et al, 2017. <strong>Algorithm: NEC.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-56" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[56]</td><td><a class="reference external" href="https://arxiv.org/abs/1702.08360">Neural Map: Structured Memory for Deep Reinforcement Learning</a>, Parisotto and Salakhutdinov, 2017. <strong>Algorithm: Neural Map.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-57" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[57]</td><td><a class="reference external" href="https://arxiv.org/abs/1803.10760">Unsupervised Predictive Memory in a Goal-Directed Agent</a>, Wayne et al, 2018. <strong>Algorithm: MERLIN.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-58" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[58]</td><td><a class="reference external" href="https://arxiv.org/abs/1806.01822">Relational Recurrent Neural Networks</a>, Santoro et al, 2018. <strong>Algorithm: RMC.</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="section-4">
<h2><a class="toc-backref" href="#toc-entry-7">6. 有模型强化学习</a><a class="headerlink" href="#section-4" title="Permalink to this headline">¶</a></h2>
<div class="section" id="a">
<h3>a. 模型可被学习<a class="headerlink" href="#a" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="footnote-59" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[59]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.06203">Imagination-Augmented Agents for Deep Reinforcement Learning</a>, Weber et al, 2017. <strong>Algorithm: I2A.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-60" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[60]</td><td><a class="reference external" href="https://arxiv.org/abs/1708.02596">Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning</a>, Nagabandi et al, 2017. <strong>Algorithm: MBMF.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-61" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[61]</td><td><a class="reference external" href="https://arxiv.org/abs/1803.00101">Model-Based Value Expansion for Efficient Model-Free Reinforcement Learning</a>, Feinberg et al, 2018. <strong>Algorithm: MVE.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-62" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[62]</td><td><a class="reference external" href="https://arxiv.org/abs/1807.01675">Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion</a>, Buckman et al, 2018. <strong>Algorithm: STEVE.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-63" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[63]</td><td><a class="reference external" href="https://openreview.net/forum?id=SJJinbWRZ&amp;noteId=SJJinbWRZ">Model-Ensemble Trust-Region Policy Optimization</a>, Kurutach et al, 2018. <strong>Algorithm: ME-TRPO.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-64" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[64]</td><td><a class="reference external" href="https://arxiv.org/abs/1809.05214">Model-Based Reinforcement Learning via Meta-Policy Optimization</a>, Clavera et al, 2018. <strong>Algorithm: MB-MPO.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-65" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[65]</td><td><a class="reference external" href="https://arxiv.org/abs/1809.01999">Recurrent World Models Facilitate Policy Evolution</a>, Ha and Schmidhuber, 2018.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="b-2">
<h3>b. 模型已知<a class="headerlink" href="#b-2" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="footnote-66" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[66]</td><td><a class="reference external" href="https://arxiv.org/abs/1712.01815">Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</a>, Silver et al, 2017. <strong>Algorithm: AlphaZero.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-67" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[67]</td><td><a class="reference external" href="https://arxiv.org/abs/1705.08439">Thinking Fast and Slow with Deep Learning and Tree Search</a>, Anthony et al, 2017. <strong>Algorithm: ExIt.</strong></td></tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="meta-rl">
<h2><a class="toc-backref" href="#toc-entry-8">7. 元学习(Meta-RL)</a><a class="headerlink" href="#meta-rl" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="footnote-68" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[68]</td><td><a class="reference external" href="https://arxiv.org/abs/1611.02779">RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning</a>, Duan et al, 2016. <strong>Algorithm: RL^2.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-69" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[69]</td><td><a class="reference external" href="https://arxiv.org/abs/1611.05763">Learning to Reinforcement Learn</a>, Wang et al, 2016.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-70" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[70]</td><td><a class="reference external" href="https://arxiv.org/abs/1703.03400">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a>, Finn et al, 2017. <strong>Algorithm: MAML.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-71" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[71]</td><td><a class="reference external" href="https://openreview.net/forum?id=B1DmUzWAW&amp;noteId=B1DmUzWAW">A Simple Neural Attentive Meta-Learner</a>, Mishra et al, 2018. <strong>Algorithm: SNAIL.</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="scaling-rl">
<h2><a class="toc-backref" href="#toc-entry-9">8. Scaling RL</a><a class="headerlink" href="#scaling-rl" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="footnote-72" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[72]</td><td><a class="reference external" href="https://arxiv.org/abs/1803.02811">Accelerated Methods for Deep Reinforcement Learning</a>, Stooke and Abbeel, 2018. <strong>Contribution:</strong> Systematic analysis of parallelization in deep RL across algorithms.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-73" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[73]</td><td><a class="reference external" href="https://arxiv.org/abs/1802.01561">IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures</a>, Espeholt et al, 2018. <strong>Algorithm: IMPALA.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-74" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[74]</td><td><a class="reference external" href="https://openreview.net/forum?id=H1Dy---0Z">Distributed Prioritized Experience Replay</a>, Horgan et al, 2018. <strong>Algorithm: Ape-X.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-75" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[75]</td><td><a class="reference external" href="https://openreview.net/forum?id=r1lyTjAqYX">Recurrent Experience Replay in Distributed Reinforcement Learning</a>, Anonymous, 2018. <strong>Algorithm: R2D2.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-76" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[76]</td><td><a class="reference external" href="https://arxiv.org/abs/1712.09381">RLlib: Abstractions for Distributed Reinforcement Learning</a>, Liang et al, 2017. <strong>Contribution:</strong> A scalable library of RL algorithm implementations. <a class="reference external" href="https://ray.readthedocs.io/en/latest/rllib.html">Documentation link.</a></td></tr>
</tbody>
</table>
</div>
<div class="section" id="section-5">
<h2><a class="toc-backref" href="#toc-entry-10">9. 现实世界的强化学习</a><a class="headerlink" href="#section-5" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="footnote-77" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[77]</td><td><a class="reference external" href="https://arxiv.org/abs/1809.07731">Benchmarking Reinforcement Learning Algorithms on Real-World Robots</a>, Mahmood et al, 2018.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-78" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[78]</td><td><a class="reference external" href="https://arxiv.org/abs/1808.00177">Learning Dexterous In-Hand Manipulation</a>, OpenAI, 2018.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-79" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[79]</td><td><a class="reference external" href="https://arxiv.org/abs/1806.10293">QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation</a>, Kalashnikov et al, 2018. <strong>Algorithm: QT-Opt.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-80" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[80]</td><td><a class="reference external" href="https://arxiv.org/abs/1811.00260">Horizon: Facebook’s Open Source Applied Reinforcement Learning Platform</a>, Gauci et al, 2018.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="section-6">
<h2><a class="toc-backref" href="#toc-entry-11">10. 安全</a><a class="headerlink" href="#section-6" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="footnote-81" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[81]</td><td><a class="reference external" href="https://arxiv.org/abs/1606.06565">Concrete Problems in AI Safety</a>, Amodei et al, 2016. <strong>Contribution:</strong> establishes a taxonomy of safety problems, serving as an important jumping-off point for future research. We need to solve these!</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-82" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[82]</td><td><a class="reference external" href="https://arxiv.org/abs/1706.03741">Deep Reinforcement Learning From Human Preferences</a>, Christiano et al, 2017. <strong>Algorithm: LFP.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-83" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[83]</td><td><a class="reference external" href="https://arxiv.org/abs/1705.10528">Constrained Policy Optimization</a>, Achiam et al, 2017. <strong>Algorithm: CPO.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-84" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[84]</td><td><a class="reference external" href="https://arxiv.org/abs/1801.08757">Safe Exploration in Continuous Action Spaces</a>, Dalal et al, 2018. <strong>Algorithm: DDPG+Safety Layer.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-85" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[85]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.05173">Trial without Error: Towards Safe Reinforcement Learning via Human Intervention</a>, Saunders et al, 2017. <strong>Algorithm: HIRL.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-86" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[86]</td><td><a class="reference external" href="https://arxiv.org/abs/1711.06782">Leave No Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning</a>, Eysenbach et al, 2017. <strong>Algorithm: Leave No Trace.</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="section-7">
<h2><a class="toc-backref" href="#toc-entry-12">11. 模仿学习和逆强化学习</a><a class="headerlink" href="#section-7" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="footnote-87" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[87]</td><td><a class="reference external" href="http://www.cs.cmu.edu/~bziebart/publications/thesis-bziebart.pdf">Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy</a>, Ziebart 2010. <strong>Contributions:</strong> Crisp formulation of maximum entropy IRL.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-88" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[88]</td><td><a class="reference external" href="https://arxiv.org/abs/1603.00448">Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization</a>, Finn et al, 2016. <strong>Algorithm: GCL.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-89" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[89]</td><td><a class="reference external" href="https://arxiv.org/abs/1606.03476">Generative Adversarial Imitation Learning</a>, Ho and Ermon, 2016. <strong>Algorithm: GAIL.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-90" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[90]</td><td><a class="reference external" href="https://xbpeng.github.io/projects/DeepMimic/2018_TOG_DeepMimic.pdf">DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills</a>, Peng et al, 2018. <strong>Algorithm: DeepMimic.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-91" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[91]</td><td><a class="reference external" href="https://arxiv.org/abs/1810.00821">Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow</a>, Peng et al, 2018. <strong>Algorithm: VAIL.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-92" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[92]</td><td><a class="reference external" href="https://arxiv.org/abs/1810.05017">One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL</a>, Le Paine et al, 2018. <strong>Algorithm: MetaMimic.</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="section-8">
<h2><a class="toc-backref" href="#toc-entry-13">12. 可复现、分析和评价</a><a class="headerlink" href="#section-8" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="footnote-93" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[93]</td><td><a class="reference external" href="https://arxiv.org/abs/1604.06778">Benchmarking Deep Reinforcement Learning for Continuous Control</a>, Duan et al, 2016. <strong>Contribution: rllab.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-94" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[94]</td><td><a class="reference external" href="https://arxiv.org/abs/1708.04133">Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control</a>, Islam et al, 2017.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-95" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[95]</td><td><a class="reference external" href="https://arxiv.org/abs/1709.06560">Deep Reinforcement Learning that Matters</a>, Henderson et al, 2017.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-96" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[96]</td><td><a class="reference external" href="https://arxiv.org/abs/1810.02525">Where Did My Optimum Go?: An Empirical Analysis of Gradient Descent Optimization in Policy Gradient Methods</a>, Henderson et al, 2018.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-97" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[97]</td><td><a class="reference external" href="https://arxiv.org/abs/1811.02553">Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms?</a>, Ilyas et al, 2018.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-98" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[98]</td><td><a class="reference external" href="https://arxiv.org/abs/1803.07055">Simple Random Search Provides a Competitive Approach to Reinforcement Learning</a>, Mania et al, 2018.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="section-9">
<h2><a class="toc-backref" href="#toc-entry-14">13. 额外奖励：强化学习理论的经典论文</a><a class="headerlink" href="#section-9" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="footnote-99" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[99]</td><td><a class="reference external" href="https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf">Policy Gradient Methods for Reinforcement Learning with Function Approximation</a>, Sutton et al, 2000. <strong>Contributions:</strong> Established policy gradient theorem and showed convergence of policy gradient algorithm for arbitrary policy classes.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-100" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[100]</td><td><a class="reference external" href="http://web.mit.edu/jnt/www/Papers/J063-97-bvr-td.pdf">An Analysis of Temporal-Difference Learning with Function Approximation</a>, Tsitsiklis and Van Roy, 1997. <strong>Contributions:</strong> Variety of convergence results and counter-examples for value-learning methods in RL.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-101" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[101]</td><td><a class="reference external" href="http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/Neural-Netw-2008-21-682_4867%5b0%5d.pdf">Reinforcement Learning of Motor Skills with Policy Gradients</a>, Peters and Schaal, 2008. <strong>Contributions:</strong> Thorough review of policy gradient methods at the time, many of which are still serviceable descriptions of deep RL methods.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-102" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[102]</td><td><a class="reference external" href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf">Approximately Optimal Approximate Reinforcement Learning</a>, Kakade and Langford, 2002. <strong>Contributions:</strong> Early roots for monotonic improvement theory, later leading to theoretical justification for TRPO and other algorithms.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-103" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[103]</td><td><a class="reference external" href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf">A Natural Policy Gradient</a>, Kakade, 2002. <strong>Contributions:</strong> Brought natural gradients into RL, later leading to TRPO, ACKTR, and several other methods in deep RL.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-104" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[104]</td><td><a class="reference external" href="https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">Algorithms for Reinforcement Learning</a>, Szepesvari, 2009. <strong>Contributions:</strong> Unbeatable reference on RL before deep RL, containing foundations and theoretical background.</td></tr>
</tbody>
</table>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="exercises.html" class="btn btn-neutral float-right" title="练习" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="spinningup.html" class="btn btn-neutral" title="深度强化学习研究者的资料" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, OpenAI.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="../_static/es5/tex-chtml.js"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>